#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
R2 CDN æ¸…ç†å’Œä¼˜åŒ–è„šæœ¬
1. æ¸…ç†å†—ä½™æ–‡ä»¶
2. åªä¿ç•™çœŸæ­£éœ€è¦CDNåŠ é€Ÿçš„å¤§æ–‡ä»¶
3. å»é‡ç›¸åŒæ–‡ä»¶
4. ä¼˜åŒ–æ–‡ä»¶ç»„ç»‡ç»“æ„
"""

import os
import sys
import json
import subprocess
import hashlib
from pathlib import Path
from collections import defaultdict
from dotenv import load_dotenv

load_dotenv()

R2_ENDPOINT = os.getenv('R2_ENDPOINT')
R2_BUCKET = os.getenv('R2_BUCKET')

def run_aws_cmd(cmd):
    """æ‰§è¡ŒAWS CLIå‘½ä»¤"""
    full_cmd = f"{cmd} --profile r2-cuhkstudy --endpoint-url {R2_ENDPOINT}"
    try:
        result = subprocess.run(full_cmd, shell=True, capture_output=True, text=True, check=True)
        return result.stdout.strip()
    except subprocess.CalledProcessError as e:
        print(f"âŒ å‘½ä»¤å¤±è´¥: {cmd}")
        print(f"é”™è¯¯: {e.stderr}")
        return None

def get_file_size(file_path):
    """è·å–æ–‡ä»¶å¤§å°(MB)"""
    try:
        return os.path.getsize(file_path) / 1024 / 1024
    except:
        return 0

def calculate_md5(file_path):
    """è®¡ç®—æ–‡ä»¶MD5"""
    try:
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except:
        return None

def clean_r2_bucket():
    """æ¸…ç†R2å­˜å‚¨æ¡¶ä¸­çš„å†—ä½™æ–‡ä»¶"""
    print("ğŸ§¹ å¼€å§‹æ¸…ç†R2å­˜å‚¨æ¡¶...")
    
    # è·å–æ‰€æœ‰æ–‡ä»¶åˆ—è¡¨
    list_cmd = f"aws s3 ls s3://{R2_BUCKET}/ --recursive"
    files_output = run_aws_cmd(list_cmd)
    
    if not files_output:
        print("âŒ æ— æ³•è·å–R2æ–‡ä»¶åˆ—è¡¨")
        return False
    
    # éœ€è¦åˆ é™¤çš„è·¯å¾„æ¨¡å¼
    delete_patterns = [
        "resource/",      # å¼ƒç”¨ç›®å½•
        "resources/",     # å¼ƒç”¨ç›®å½•  
        "Uploads/",       # ç”¨æˆ·ä¸Šä¼ ç›®å½•ï¼Œä¸éœ€è¦CDN
        "themes/blowfish/exampleSite/",  # ç¤ºä¾‹æ–‡ä»¶
        "themes/blowfish/assets/",       # ä¸»é¢˜èµ„æº
        "themes/blowfish/static/",       # ä¸»é¢˜é™æ€æ–‡ä»¶
        "content/",       # å†…å®¹æºæ–‡ä»¶
    ]
    
    files_to_delete = []
    
    for line in files_output.split('\n'):
        if not line.strip():
            continue
        
        # è§£ææ–‡ä»¶ä¿¡æ¯
        parts = line.split()
        if len(parts) < 4:
            continue
            
        file_key = parts[3]
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ é™¤
        should_delete = False
        for pattern in delete_patterns:
            if file_key.startswith(pattern):
                should_delete = True
                break
        
        # æ£€æŸ¥å°æ–‡ä»¶ (< 100KB)
        if not should_delete:
            file_size_bytes = int(parts[2])
            if file_size_bytes < 100 * 1024:  # å°äº100KB
                # åªä¿ç•™PDFæ–‡ä»¶ï¼Œåˆ é™¤å°å›¾æ ‡ç­‰
                if not file_key.endswith('.pdf'):
                    should_delete = True
        
        if should_delete:
            files_to_delete.append(file_key)
    
    print(f"ğŸ“‹ å‘ç° {len(files_to_delete)} ä¸ªæ–‡ä»¶éœ€è¦åˆ é™¤")
    
    # æ‰¹é‡åˆ é™¤
    if files_to_delete:
        print("ğŸ—‘ï¸  å¼€å§‹æ‰¹é‡åˆ é™¤æ–‡ä»¶...")
        
        # æ¯æ¬¡åˆ é™¤1000ä¸ªæ–‡ä»¶
        batch_size = 1000
        for i in range(0, len(files_to_delete), batch_size):
            batch = files_to_delete[i:i+batch_size]
            
            # åˆ›å»ºåˆ é™¤æ¸…å•æ–‡ä»¶
            delete_list = "delete-list.json"
            delete_objects = {
                "Objects": [{"Key": key} for key in batch],
                "Quiet": True
            }
            
            with open(delete_list, 'w') as f:
                json.dump(delete_objects, f)
            
            # æ‰§è¡Œåˆ é™¤
            delete_cmd = f"aws s3api delete-objects --bucket {R2_BUCKET} --delete file://{delete_list}"
            if run_aws_cmd(delete_cmd):
                print(f"  âœ… åˆ é™¤äº† {len(batch)} ä¸ªæ–‡ä»¶")
            else:
                print(f"  âŒ åˆ é™¤å¤±è´¥")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.remove(delete_list)
    
    print("âœ… R2æ¸…ç†å®Œæˆ")
    return True

def find_large_files():
    """æŸ¥æ‰¾éœ€è¦CDNçš„å¤§æ–‡ä»¶"""
    print("ğŸ” åˆ†æéœ€è¦CDNçš„å¤§æ–‡ä»¶...")
    
    target_dirs = [
        "/var/www/cuhkstudy/static",
        "/var/www/cuhkstudy/assets", 
        "/var/www/cuhkstudy/public"
    ]
    
    large_files = []
    duplicates = defaultdict(list)
    
    for base_dir in target_dirs:
        if not os.path.exists(base_dir):
            continue
            
        print(f"ğŸ“‚ æ‰«æ {base_dir}")
        
        for root, dirs, files in os.walk(base_dir):
            for file in files:
                file_path = os.path.join(root, file)
                file_size = get_file_size(file_path)
                
                # åªå¤„ç†å¤§æ–‡ä»¶æˆ–PDF
                if file_size > 0.1 or file.endswith(('.pdf', '.PDF')):  # >100KB æˆ– PDF
                    file_md5 = calculate_md5(file_path)
                    if file_md5:
                        rel_path = os.path.relpath(file_path, "/var/www/cuhkstudy")
                        
                        large_files.append({
                            'path': file_path,
                            'rel_path': rel_path,
                            'size_mb': file_size,
                            'md5': file_md5,
                            'ext': Path(file).suffix.lower()
                        })
                        
                        duplicates[file_md5].append(rel_path)
    
    # è¾“å‡ºé‡å¤æ–‡ä»¶æŠ¥å‘Š
    print("\nğŸ“Š é‡å¤æ–‡ä»¶åˆ†æ:")
    for md5, paths in duplicates.items():
        if len(paths) > 1:
            print(f"  ğŸ”„ MD5: {md5[:8]}... æœ‰ {len(paths)} ä¸ªå‰¯æœ¬:")
            for path in paths:
                print(f"    - {path}")
    
    return large_files, duplicates

def create_optimized_upload_list(large_files, duplicates):
    """åˆ›å»ºä¼˜åŒ–çš„ä¸Šä¼ åˆ—è¡¨"""
    print("ğŸ“ åˆ›å»ºä¼˜åŒ–ä¸Šä¼ ç­–ç•¥...")
    
    upload_list = []
    processed_md5 = set()
    
    # ä¼˜å…ˆçº§è§„åˆ™
    priority_order = [
        'static/',      # æœ€é«˜ä¼˜å…ˆçº§
        'assets/',      # æ¬¡ä¼˜å…ˆçº§  
        'public/',      # æœ€ä½ä¼˜å…ˆçº§
    ]
    
    for file_info in large_files:
        md5 = file_info['md5']
        
        # è·³è¿‡å·²å¤„ç†çš„é‡å¤æ–‡ä»¶
        if md5 in processed_md5:
            continue
        
        # å¯¹äºé‡å¤æ–‡ä»¶ï¼Œé€‰æ‹©æœ€ä¼˜è·¯å¾„
        if len(duplicates[md5]) > 1:
            best_path = None
            best_priority = 999
            
            for dup_path in duplicates[md5]:
                for i, prefix in enumerate(priority_order):
                    if dup_path.startswith(prefix):
                        if i < best_priority:
                            best_priority = i
                            best_path = dup_path
                        break
            
            if best_path:
                # æ‰¾åˆ°å¯¹åº”çš„file_info
                for f in large_files:
                    if f['rel_path'] == best_path:
                        upload_list.append(f)
                        processed_md5.add(md5)
                        break
        else:
            upload_list.append(file_info)
            processed_md5.add(md5)
    
    return upload_list

def upload_optimized_files(upload_list):
    """ä¸Šä¼ ä¼˜åŒ–åçš„æ–‡ä»¶åˆ—è¡¨"""
    print(f"ğŸ“¤ å¼€å§‹ä¸Šä¼  {len(upload_list)} ä¸ªä¼˜åŒ–æ–‡ä»¶...")
    
    success_count = 0
    total_size = 0
    
    for file_info in upload_list:
        file_path = file_info['path']
        rel_path = file_info['rel_path']
        size_mb = file_info['size_mb']
        
        # ä¼˜åŒ–S3 keyè·¯å¾„
        s3_key = rel_path
        if s3_key.startswith('public/'):
            s3_key = s3_key[7:]  # å»æ‰ public/ å‰ç¼€
        
        print(f"ğŸ“¤ {file_path} -> s3://{R2_BUCKET}/{s3_key} ({size_mb:.2f}MB)")
        
        upload_cmd = f"""aws s3 cp "{file_path}" "s3://{R2_BUCKET}/{s3_key}" \
            --content-type "$(file -b --mime-type '{file_path}')" \
            --cache-control "public, max-age=31536000" \
            --acl public-read"""
        
        if run_aws_cmd(upload_cmd):
            success_count += 1
            total_size += size_mb
        else:
            print(f"  âŒ ä¸Šä¼ å¤±è´¥: {rel_path}")
    
    print(f"\nğŸ“Š ä¸Šä¼ ç»Ÿè®¡:")
    print(f"  âœ… æˆåŠŸ: {success_count}/{len(upload_list)} ä¸ªæ–‡ä»¶")
    print(f"  ğŸ“¦ æ€»å¤§å°: {total_size:.2f} MB")
    
    return success_count == len(upload_list)

def clean_deprecated_dirs():
    """æ¸…ç†æœ¬åœ°å¼ƒç”¨ç›®å½•"""
    print("ğŸ—‚ï¸  æ¸…ç†æœ¬åœ°å¼ƒç”¨ç›®å½•...")
    
    deprecated_dirs = [
        "/var/www/cuhkstudy/resource",
        "/var/www/cuhkstudy/resources", 
        "/root/cuhkstudy/resource",
        "/root/cuhkstudy/resources"
    ]
    
    for dir_path in deprecated_dirs:
        if os.path.exists(dir_path):
            print(f"ğŸ—‘ï¸  åˆ é™¤ç›®å½•: {dir_path}")
            # å…ˆç§»åŠ¨åˆ°ä¸´æ—¶ä½ç½®ï¼Œç¡®è®¤åå†åˆ é™¤
            backup_path = f"{dir_path}.backup"
            os.rename(dir_path, backup_path)
            print(f"  ğŸ“ å·²ç§»åŠ¨åˆ°: {backup_path}")
            print(f"  âš ï¸  è¯·ç¡®è®¤æ— é—®é¢˜åæ‰‹åŠ¨åˆ é™¤: rm -rf {backup_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹R2 CDNä¼˜åŒ–...")
    
    # 1. æ¸…ç†R2å­˜å‚¨æ¡¶
    if not clean_r2_bucket():
        return 1
    
    # 2. åˆ†ææœ¬åœ°å¤§æ–‡ä»¶
    large_files, duplicates = find_large_files()
    
    # 3. åˆ›å»ºä¼˜åŒ–ä¸Šä¼ åˆ—è¡¨
    upload_list = create_optimized_upload_list(large_files, duplicates)
    
    # 4. ä¸Šä¼ ä¼˜åŒ–æ–‡ä»¶
    if not upload_optimized_files(upload_list):
        return 1
    
    # 5. æ¸…ç†å¼ƒç”¨ç›®å½•
    clean_deprecated_dirs()
    
    # 6. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    print("\nğŸ‰ ä¼˜åŒ–å®Œæˆ!")
    print("ğŸ“‹ ä¸‹ä¸€æ­¥å»ºè®®:")
    print("  1. æµ‹è¯•ç½‘ç«™åŠŸèƒ½æ˜¯å¦æ­£å¸¸")
    print("  2. ç¡®è®¤å¼ƒç”¨ç›®å½•å¤‡ä»½ååˆ é™¤")
    print("  3. æ›´æ–°nginxé…ç½®ä»¥åŒ¹é…æ–°çš„æ–‡ä»¶ç»“æ„")
    
    return 0

if __name__ == "__main__":
    sys.exit(main())