#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Cloudflare R2 æ‰¹é‡ä¸Šä¼ è„šæœ¬
ä¸Šä¼ ç°æœ‰çš„PDFå’Œå›¾ç‰‡æ–‡ä»¶åˆ°R2å­˜å‚¨æ¡¶
"""

import os
import sys
import json
import mimetypes
import concurrent.futures
from pathlib import Path
from dotenv import load_dotenv
import boto3
from botocore.exceptions import ClientError
import hashlib

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# R2é…ç½®
R2_ACCOUNT_ID = os.getenv('R2_ACCOUNT_ID')
R2_ENDPOINT = os.getenv('R2_ENDPOINT')
R2_BUCKET = os.getenv('R2_BUCKET')
R2_ADMIN_ACCESS_KEY = os.getenv('R2_ADMIN_ACCESS_KEY')
R2_ADMIN_SECRET_KEY = os.getenv('R2_ADMIN_SECRET_KEY')

def get_r2_client():
    """åˆ›å»ºR2 S3å®¢æˆ·ç«¯"""
    return boto3.client(
        's3',
        endpoint_url=R2_ENDPOINT,
        aws_access_key_id=R2_ADMIN_ACCESS_KEY,
        aws_secret_access_key=R2_ADMIN_SECRET_KEY,
        region_name='auto'
    )

def calculate_md5(file_path):
    """è®¡ç®—æ–‡ä»¶MD5"""
    hash_md5 = hashlib.md5()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()

def get_relative_key(file_path, base_path="/var/www/cuhkstudy"):
    """è·å–ç›¸å¯¹è·¯å¾„ä½œä¸ºR2å¯¹è±¡é”®"""
    return str(Path(file_path).relative_to(base_path))

def upload_single_file(args):
    """ä¸Šä¼ å•ä¸ªæ–‡ä»¶"""
    file_path, s3_client = args
    
    try:
        # è·å–æ–‡ä»¶ä¿¡æ¯
        key = get_relative_key(file_path)
        content_type = mimetypes.guess_type(file_path)[0] or 'application/octet-stream'
        file_size = os.path.getsize(file_path)
        
        print(f"ä¸Šä¼ : {file_path} -> s3://{R2_BUCKET}/{key}")
        
        # ä¸Šä¼ æ–‡ä»¶
        with open(file_path, 'rb') as f:
            s3_client.put_object(
                Bucket=R2_BUCKET,
                Key=key,
                Body=f,
                ContentType=content_type,
                ACL='public-read'  # è®¾ç½®ä¸ºå…¬å…±å¯è¯»
            )
        
        # ç”Ÿæˆå…¬å…±URL
        public_url = f"{R2_ENDPOINT}/{R2_BUCKET}/{key}"
        
        return {
            'success': True,
            'local_path': file_path,
            'r2_key': key,
            'public_url': public_url,
            'size': file_size,
            'content_type': content_type
        }
        
    except Exception as e:
        return {
            'success': False,
            'local_path': file_path,
            'error': str(e)
        }

def main():
    print("ğŸš€ å¼€å§‹æ‰¹é‡ä¸Šä¼ æ–‡ä»¶åˆ° Cloudflare R2...")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not all([R2_ACCOUNT_ID, R2_ENDPOINT, R2_BUCKET, R2_ADMIN_ACCESS_KEY, R2_ADMIN_SECRET_KEY]):
        print("âŒ ç¼ºå°‘å¿…è¦çš„ç¯å¢ƒå˜é‡ï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶")
        sys.exit(1)
    
    # è¯»å–æ–‡ä»¶åˆ—è¡¨
    files_list_path = 'files_to_upload.txt'
    if not os.path.exists(files_list_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶åˆ—è¡¨: {files_list_path}")
        sys.exit(1)
    
    with open(files_list_path, 'r') as f:
        files_to_upload = [line.strip() for line in f if line.strip()]
    
    print(f"ğŸ“ æ‰¾åˆ° {len(files_to_upload)} ä¸ªæ–‡ä»¶éœ€è¦ä¸Šä¼ ")
    
    # åˆ›å»ºS3å®¢æˆ·ç«¯
    s3_client = get_r2_client()
    
    # æµ‹è¯•è¿æ¥
    try:
        s3_client.head_bucket(Bucket=R2_BUCKET)
        print(f"âœ… æˆåŠŸè¿æ¥åˆ° R2 å­˜å‚¨æ¡¶: {R2_BUCKET}")
    except ClientError as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ° R2: {e}")
        sys.exit(1)
    
    # å¹¶å‘ä¸Šä¼ 
    upload_results = []
    failed_uploads = []
    
    print(f"ğŸ”„ å¼€å§‹ä¸Šä¼ ï¼Œä½¿ç”¨ 10 ä¸ªå¹¶å‘çº¿ç¨‹...")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºå‚æ•°å…ƒç»„
        upload_args = [(file_path, get_r2_client()) for file_path in files_to_upload]
        
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_file = {executor.submit(upload_single_file, args): args[0] for args in upload_args}
        
        # å¤„ç†ç»“æœ
        for future in concurrent.futures.as_completed(future_to_file):
            result = future.result()
            upload_results.append(result)
            
            if result['success']:
                print(f"âœ… {result['local_path']} -> {result['public_url']}")
            else:
                print(f"âŒ {result['local_path']}: {result['error']}")
                failed_uploads.append(result)
    
    # ç”Ÿæˆä¸Šä¼ æ˜ å°„è¡¨
    successful_uploads = [r for r in upload_results if r['success']]
    
    mapping = {}
    for result in successful_uploads:
        mapping[result['local_path']] = {
            'r2_key': result['r2_key'],
            'public_url': result['public_url'],
            'size': result['size'],
            'content_type': result['content_type']
        }
    
    # ä¿å­˜ç»“æœ
    with open('upload_mapping.json', 'w', encoding='utf-8') as f:
        json.dump(mapping, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜å¤±è´¥åˆ—è¡¨
    if failed_uploads:
        with open('failed_uploads.json', 'w', encoding='utf-8') as f:
            json.dump(failed_uploads, f, indent=2, ensure_ascii=False)
    
    # è¾“å‡ºç»Ÿè®¡
    print(f"\nğŸ“Š ä¸Šä¼ å®Œæˆç»Ÿè®¡:")
    print(f"   âœ… æˆåŠŸ: {len(successful_uploads)} ä¸ªæ–‡ä»¶")
    print(f"   âŒ å¤±è´¥: {len(failed_uploads)} ä¸ªæ–‡ä»¶")
    print(f"   ğŸ“ æ˜ å°„è¡¨å·²ä¿å­˜åˆ°: upload_mapping.json")
    
    if failed_uploads:
        print(f"   âš ï¸  å¤±è´¥åˆ—è¡¨å·²ä¿å­˜åˆ°: failed_uploads.json")
    
    # è®¡ç®—æ€»å¤§å°
    total_size = sum(r['size'] for r in successful_uploads)
    print(f"   ğŸ“¦ æ€»ä¸Šä¼ å¤§å°: {total_size / 1024 / 1024:.2f} MB")

if __name__ == "__main__":
    main()